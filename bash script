#!/bin/bash

# Check if the database name argument is provided
if [ -z "$1" ]; then
  echo "Usage: $0 <database_name>"
  exit 1
fi

# Set the Hive database name
database=$1

# Define the output file name
output_file="${database}_ddl.hql"

# Extract the DDL of the Hive database
hive -e "SHOW CREATE DATABASE $database;" > $output_file

echo "DDL for database $database extracted to $output_file"

Get the database name as an argument
if [ -z "$1" ]
  then
    echo "Please provide a database name as an argument."
    exit 1
fi

DB_NAME=$1

# Set the output file name
OUTPUT_FILE=${DB_NAME}_DDL.sql

# Run the Hive query to extract the DDL for all tables in the specified database
hive -e "SHOW TABLES IN ${DB_NAME};" | while read table; do
  hive -e "SHOW CREATE TABLE ${DB_NAME}.${table};"
done > $OUTPUT_FILE

echo "DDL extracted for database ${DB_NAME} and saved to ${OUTPUT_FILE}."

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._

object HiveToOzonePOC {

  def main(args: Array[String]): Unit = {

    // Create a SparkSession
    val spark = SparkSession.builder()
      .appName("HiveToOzonePOC")
      .enableHiveSupport()
      .getOrCreate()

    // Read data from a Hive table
    val hiveDF = spark.sql("SELECT * FROM <hive_database>.<hive_table>")

    // Transform the data
    val transformedDF = hiveDF.select(col("<hive_column_1>"), col("<hive_column_2>"))

    // Save the transformed data to Ozone
    transformedDF.write
      .format("org.apache.hadoop.fs.ozone.BasicOzoneFileSink")
      .option("ozone.om.address", "<ozone_om_address>")
      .option("ozone.om.user.key", "<ozone_om_user_key>")
      .option("ozone.om.user.value", "<ozone_om_user_value>")
      .option("ozone.volume.name", "<ozone_volume_name>")
      .option("ozone.bucket.name", "<ozone_bucket_name>")
      .option("ozone.key.name", "<ozone_key_name>")
      .mode("overwrite")
      .save()

    // Stop the

import org.apache.spark.sql.{SparkSession, SQLContext}

object ReadAllHiveTables {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder()
      .appName("ReadAllHiveTables")
      .enableHiveSupport()
      .getOrCreate()

    val sqlContext = spark.sqlContext
    import sqlContext.implicits._

    val tables = spark.catalog.listTables()
    tables.foreach(table => {
      val tableName = table.name
      val tableDF = spark.table(tableName)
      tableDF.show()
    })

    spark.stop()
  }
}
